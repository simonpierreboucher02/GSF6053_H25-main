\documentclass[14pt]{extarticle} % Utilisation de extarticle pour supporter 14pt

% ------------------------------------------------------------------------
% Packages indispensables et recommandés
% ------------------------------------------------------------------------
\usepackage[utf8]{inputenc}     % Encodage des caractères
\usepackage[T1]{fontenc}        % Encodage de la police
\usepackage[french]{babel}      % Support de la langue française
\usepackage{amsmath, amssymb}   % Environnement mathématique enrichi
\usepackage{graphicx}           % Inclusion d'images
\usepackage{hyperref}           % Liens hypertextes
\usepackage{geometry}           % Gestion des marges
\usepackage{titlesec}           % Personnalisation des titres
\usepackage{setspace}           % Gestion de l'interligne
\usepackage{xcolor}             % Gestion des couleurs
\usepackage{lipsum}             % (optionnel) Pour générer du faux texte
\usepackage{booktabs}           % (optionnel) Pour améliorer les tableaux
\usepackage{fancyhdr}           % Personnalisation des en-têtes et pieds de page
\usepackage{cleveref}           % Références intelligentes
\usepackage{caption}            % Pour personnaliser les légendes

% ------------------------------------------------------------------------
% Configuration de la mise en page
% ------------------------------------------------------------------------
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm
}

% ------------------------------------------------------------------------
% Personnalisation des sections
% ------------------------------------------------------------------------
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ------------------------------------------------------------------------
% Commande pour mettre en avant les livres en bleu
% ------------------------------------------------------------------------
\newcommand{\livre}[1]{\textcolor{blue}{#1}}

% ------------------------------------------------------------------------
% Configuration des hyperliens
% ------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,          % Les liens sont colorés
    linkcolor=blue,           % Couleur des liens internes (TOC, références, etc.)
    urlcolor=blue,            % Couleur des URL
    citecolor=blue,           % Couleur des citations
    filecolor=blue,           % Couleur des liens vers des fichiers
    pdfborder={0 0 0},        % Pas de bordure autour des liens
    breaklinks=true            % Permet les sauts de ligne dans les liens
}

% ------------------------------------------------------------------------
% Configuration des en-têtes et pieds de page
% ------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Moindres Carrés Ordinaires}
\fancyhead[R]{Hiver 2025}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\includegraphics[height=1cm]{logo_universite_laval.png}} % Ajout du logo en bas à droite

% ------------------------------------------------------------------------
% Informations sur le document
% ------------------------------------------------------------------------
\title{\textbf{Moindres Carrés Ordinaires}}
\author{GSF-6053}
\date{Hiver 2025}

% ------------------------------------------------------------------------
% Autoriser les coupures d'équations
% ------------------------------------------------------------------------
\allowdisplaybreaks

% ------------------------------------------------------------------------
% Début du document
% ------------------------------------------------------------------------
\begin{document}

\maketitle

\tableofcontents
\newpage

% Augmenter l'interligne à 1,5
\onehalfspacing

% ------------------------------------------------------------------------
% SECTION 1 : Introduction aux Moindres Carrés Ordinaires (MCO)
% ------------------------------------------------------------------------
\section{Introduction aux Moindres Carrés Ordinaires (MCO)}

\textbf{Chapitres 3 de \livre{Gujarati et Porter}, Chapitre 2 de \livre{Wooldridge}.}

Les Moindres Carrés Ordinaires (MCO) sont une méthode d'estimation utilisée en économétrie pour déterminer les paramètres d'un modèle de régression linéaire. L'objectif principal des MCO est de minimiser la somme des carrés des résidus, c'est-à-dire la somme des différences au carré entre les valeurs observées et les valeurs prédites par le modèle.

\subsection{Objectif des MCO}

L'objectif des MCO est d'estimer les paramètres \(\beta\) dans le modèle de régression linéaire suivant :
\[
Y = X\beta + u,
\]
où :
\begin{itemize}
    \item \(Y\) est le vecteur des variables dépendantes.
    \item \(X\) est la matrice des variables explicatives (incluant une colonne de constantes si nécessaire).
    \item \(\beta\) est le vecteur des paramètres à estimer.
    \item \(u\) est le vecteur des termes d'erreur.
\end{itemize}

\subsection{Hypothèses des MCO}

Pour que les estimateurs des MCO soient les meilleurs estimateurs linéaires sans biais (BLUE - Best Linear Unbiased Estimators), plusieurs hypothèses doivent être respectées :
\begin{enumerate}
    \item \textbf{Linéarité des Paramètres} : Le modèle doit être linéaire en ses paramètres \(\beta\).
    \item \textbf{Exogénéité des Régresseurs} : Les variables explicatives \(X\) doivent être exogènes, c'est-à-dire que \(E(u \mid X) = 0\).
    \item \textbf{Absence de Multicolinéarité Parfaite} : Les variables explicatives ne doivent pas être parfaitement corrélées entre elles.
    \item \textbf{Homoscedasticité} : La variance des termes d'erreur \(u\) doit être constante, \(\text{Var}(u \mid X) = \sigma^2 I\).
    \item \textbf{Indépendance des Termes d'Erreur} : Les termes d'erreur \(u\) doivent être indépendants entre eux.
\end{enumerate}

\subsection{Propriétés des Estimateurs MCO}

Sous les hypothèses précédentes, les estimateurs des MCO \(\hat{\beta}\) possèdent les propriétés suivantes :
\begin{itemize}
    \item \textbf{Non Biaisés} : \(E(\hat{\beta}) = \beta\).
    \item \textbf{Minimum de la Variance} : Parmi tous les estimateurs linéaires non biaisés, \(\hat{\beta}\) a la variance la plus faible.
    \item \textbf{Consistance} : Lorsque la taille de l'échantillon augmente, \(\hat{\beta}\) converge en probabilité vers \(\beta\).
    \item \textbf{Normalité} : Si les termes d'erreur \(u\) sont normalement distribués, alors \(\hat{\beta}\) suit une distribution normale.
\end{itemize}

\subsection{Importance des MCO en Econométrie}

Les MCO sont fondamentaux en économétrie car ils fournissent une méthode simple et efficace pour estimer les relations entre les variables économiques. Ils sont utilisés dans une multitude d'applications, telles que :
\begin{itemize}
    \item Estimation de l'impact des facteurs économiques sur la consommation.
    \item Analyse de la relation entre l'investissement et le taux d'intérêt.
    \item Étude de l'effet des dépenses publicitaires sur les ventes.
\end{itemize}

Les MCO servent également de base pour des méthodes plus avancées en économétrie, telles que la régression avec variables endogènes, les modèles à effets fixes, et les modèles de panel.

% ------------------------------------------------------------------------
% SECTION 2 : Régression Simple et Moindres Carrés Ordinaires : Approche en Sommation
% ------------------------------------------------------------------------
\section{Régression Simple et Moindres Carrés Ordinaires : Approche en Sommation}

\subsection{Intuition de la Régression}
\textbf{Chapitres 1 et 2 de \livre{Gujarati et Porter}, Chapitre 2 de \livre{Wooldridge}.}

En termes généraux, l’analyse par régression s’intéresse à déterminer une relation de dépendance entre une variable dépendante et une ou plusieurs variables explicatives. L’objectif est d’estimer la valeur moyenne de la variable dépendante \( E(Y) \) en termes de valeurs connues des variables explicatives ou indépendantes.

Il faut être prudent avec le vocabulaire employé. L’analyse par régression explique une dépendance, et non un lien de cause à effet. La \emph{causalité} en économétrie est une relation plus forte de sens que la dépendance. Une relation statistique trouvée dans un échantillon de données n’implique pas nécessairement un lien de causalité. Bien qu’il existe des tests statistiques de causalité (tests de causalité de Granger ou de Sims), la causalité est souvent explicite dans le modèle théorique testé.

Il faut aussi différencier la régression de la corrélation. Le coefficient de corrélation mesure la force ou le degré d’une association linéaire entre deux variables. Par exemple, on peut vouloir mesurer le coefficient de corrélation entre un titre en particulier et un indice de référence. En revanche, la régression de ce titre sur un indice de référence estime la valeur moyenne du titre pour une variation dans l’indice. Aussi, la régression traite les variables de façon asymétrique : on suppose des variables explicatives \emph{connues ou fixes}\footnote{Cette hypothèse peut être relâchée, mais les modèles deviennent plus complexes et les estimateurs ont alors des propriétés différentes du point de vue statistique.} alors que la variable dépendante est stochastique (avec une distribution de probabilité). En corrélation, les deux variables sont traitées de manière symétrique et sont toutes deux stochastiques.

\subsection{La Régression : Introduction}

On s’intéresse donc à trouver l’espérance conditionnelle d’une ou plusieurs variables dépendantes (\(Y\)) conditionnelle à la valeur connue des régresseurs (\(X\)) :
\[
E(Y \mid X_i) = f(X_i), \quad (1.1)
\]
où \( f(X_i) \) est une fonction des variables explicatives \( X \). Pour illustrer, supposons que nous avons une variable à expliquer (le rendement du titre Apple, \(Y\)) et une variable explicative (le rendement du marché, \(X_i\)). On assume que la relation entre \(Y\) et \(X\) est \textbf{linéaire} :
\[
E(Y \mid X_i) = \beta_1 + \beta_2 X_i, \quad (1.2)
\]
où \(\beta_1\) (ordonnée à l’origine) et \(\beta_2\) (pente) sont des paramètres fixes mais inconnus.

Puisque la régression explique la moyenne de \(Y\), on s’attend à des déviations entre \(Y_i\) et \(E(Y \mid X_i)\). Cette déviation ou \emph{erreur} est :
\[
u_i = Y_i - E(Y \mid X_i). \quad (1.3)
\]

En pratique, le rendement d’Apple s’écrit alors :
\[
Y_i = \beta_1 + \beta_2 X_i + u_i. \quad (1.4)
\]

En prenant l’espérance conditionnelle, on obtient :
\[
E(Y_i \mid X_i) = E(Y \mid X_i) + E(u_i).
\]
Si \(E(u_i) = 0\), alors \(E(Y_i \mid X_i) = \beta_1 + \beta_2 X_i\).

\paragraph{Pourquoi ajouter un terme d’erreur?}
\begin{enumerate}
    \item Le modèle théorique peut omettre certaines variables.
    \item Certaines variables sont impossibles à mesurer (ex. : l’aversion au risque).
    \item Il n’existe pas de relation certaine à 100\%.
    \item On privilégie des modèles parcimonieux ; les effets marginaux non spécifiés finissent dans l’erreur.
    \item La forme fonctionnelle (par ex. linéaire) est une approximation.
\end{enumerate}

Dans la pratique, on ne dispose que d’un échantillon de données, et non de la population entière. Les paramètres du modèle estimés à partir des données sont notés \(\hat{\beta}_1\) et \(\hat{\beta}_2\). Ainsi, le modèle estimé s’écrit :
\[
\hat{Y}_i = \hat{\beta}_1 + \hat{\beta}_2 X_i, \quad (1.6)
\]
\[
Y_i = \hat{\beta}_1 + \hat{\beta}_2 X_i + \hat{u}_i. \quad (1.7)
\]

Ici, \(\hat{u}_i = Y_i - \hat{Y}_i\) est le \emph{résidu}.

\subsection{Les Moindres Carrés Ordinaires (MCO)}
\textbf{Chapitres 3 de \livre{Gujarati et Porter}.}

Définissons d’abord le résidu :
\[
\hat{u}_i = Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i.
\]

L’idée des MCO est de choisir \(\hat{\beta}_1\) et \(\hat{\beta}_2\) de façon à minimiser la somme des carrés des résidus :
\[
\sum_{i=1}^{N} \hat{u}_i^2 = \sum_{i=1}^{N} (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i)^2.
\]

En dérivant cette fonction (par rapport à \(\hat{\beta}_1\) et \(\hat{\beta}_2\)) et en l’égalant à zéro, on obtient les conditions de premier ordre qui mènent aux \emph{équations normales} :
\begin{align*}
    \hat{\beta}_2 &= \frac{\sum_{i=1}^{N} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{N} (X_i - \bar{X})^2}, \\
    \hat{\beta}_1 &= \bar{Y} - \hat{\beta}_2 \bar{X}.
\end{align*}

La droite de régression estimée passe par les moyennes \(\bar{X}\) et \(\bar{Y}\). Dans la suite, on généralise cette méthode à plusieurs variables explicatives et on utilise la notation matricielle.

% ------------------------------------------------------------------------
% SECTION 3 : Dérivation Matricielle des Moindres Carrés Ordinaires
% ------------------------------------------------------------------------
\section{Dérivation Matricielle des Moindres Carrés Ordinaires}

\textbf{Chapitres 3 et 4, Appendice C de \livre{Gujarati et Porter} ; Chapitres 2 et 3, Appendice E de \livre{Wooldridge}.}

\subsection{Le Modèle}

Le modèle linéaire à \(K\) variables explicatives s’écrit :
\[
Y = X \beta + u, \quad (2.1)
\]
avec 
\[
Y = 
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_T
\end{bmatrix}, 
\quad
X = 
\begin{bmatrix}
1 & X_{11} & \dots & X_{1,K-1} \\
1 & X_{21} & \dots & X_{2,K-1} \\
\vdots & \vdots & \ddots & \vdots \\
1 & X_{T1} & \dots & X_{T,K-1}
\end{bmatrix}_{T\times K},
\quad
\beta = 
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_{K-1}
\end{bmatrix}_{K\times 1},
\quad
u = 
\begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_T
\end{bmatrix}_{T\times 1}.
\]

La composante déterministe est \(X\beta\), la composante aléatoire est \(u\). L’hypothèse clé est que \(E(u \mid X) = 0\) et que les composantes de \(u\) sont supposées iid, sans corrélation ni hétéroscédasticité.

\subsection{Hypothèses du Modèle de Régression}
\begin{enumerate}
    \item Linéarité des paramètres.
    \item Les régresseurs \(X\) sont fixes (ou exogènes).
    \item \(T > K\) et \(\mathrm{rang}(X) = K\).
    \item Pas de multicolinéarité parfaite.
\end{enumerate}

\subsection{Dérivation des Estimateurs MCO}

La méthode consiste à minimiser la somme des carrés des résidus :
\[
\hat{\beta} 
= \underset{\beta}{\arg\min} 
\bigl(Y - X \beta \bigr)' \bigl(Y - X \beta \bigr).
\]

Par dérivation et condition de premier ordre, on obtient :
\[
X'X \hat{\beta} = X'Y \quad \Longrightarrow \quad \hat{\beta} = (X'X)^{-1}X'Y.
\]

La variance du terme d’erreur peut être estimée par :
\[
\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t^2 = \frac{\hat{u}'\hat{u}}{T},
\]
où \(\hat{u} = Y - X\hat{\beta}\).

% ------------------------------------------------------------------------
% SECTION : Bibliographie
% ------------------------------------------------------------------------
\section*{Bibliographie}
\addcontentsline{toc}{section}{Bibliographie}

\begin{thebibliography}{99}

\bibitem{GujaratiPorter}
Gujarati, D. N., \& Porter, D. C. (2009). \textit{Basic Econometrics}. McGraw-Hill.

\bibitem{Wooldridge}
Wooldridge, J. M. (2010). \textit{Econometric Analysis of Cross Section and Panel Data}. MIT Press.

\bibitem{Greene}
Greene, W. H. (2012). \textit{Econometric Analysis}. Pearson.

\end{thebibliography}

\end{document}
