\documentclass[14pt]{extarticle} % Utilisation de extarticle pour supporter 14pt

% ------------------------------------------------------------------------
% Packages indispensables et recommandés
% ------------------------------------------------------------------------
\usepackage[utf8]{inputenc}     % Encodage des caractères
\usepackage[T1]{fontenc}        % Encodage de la police
\usepackage[french]{babel}      % Support de la langue française
\usepackage{amsmath, amssymb}   % Environnement mathématique enrichi
\usepackage{graphicx}           % Inclusion d'images
\usepackage{hyperref}           % Liens hypertextes
\usepackage{geometry}           % Gestion des marges
\usepackage{titlesec}           % Personnalisation des titres
\usepackage{setspace}           % Gestion de l'interligne
\usepackage{xcolor}             % Gestion des couleurs
\usepackage{lipsum}             % (optionnel) Pour générer du faux texte
\usepackage{booktabs}           % (optionnel) Pour améliorer les tableaux
\usepackage{fancyhdr}           % Personnalisation des en-têtes et pieds de page
\usepackage{cleveref}           % Références intelligentes
\usepackage{caption}            % Pour personnaliser les légendes

% ------------------------------------------------------------------------
% Configuration de la mise en page
% ------------------------------------------------------------------------
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm
}

% ------------------------------------------------------------------------
% Personnalisation des sections
% ------------------------------------------------------------------------
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ------------------------------------------------------------------------
% Commande pour mettre en avant les livres en bleu
% ------------------------------------------------------------------------
\newcommand{\livre}[1]{\textcolor{blue}{#1}}

% ------------------------------------------------------------------------
% Configuration des hyperliens
% ------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,          % Les liens sont colorés
    linkcolor=blue,           % Couleur des liens internes (TOC, références, etc.)
    urlcolor=blue,            % Couleur des URL
    citecolor=blue,           % Couleur des citations
    filecolor=blue,           % Couleur des liens vers des fichiers
    pdfborder={0 0 0},        % Pas de bordure autour des liens
    breaklinks=true            % Permet les sauts de ligne dans les liens
}
% ------------------------------------------------------------------------
% Configuration des en-têtes et pieds de page
% ------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Preuve des MCO en Format Matriciel}
\fancyhead[R]{Hiver 2025}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\includegraphics[height=1cm]{logo_universite_laval.png}} % Ajout du logo ERS en bas à droite


\title{Preuve des Moindres Carrés Ordinaires (MCO) en Format Matriciel}
\author{Simon-Pierre Boucher}
\date{Janvier 2025}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
Dans cette section, nous démontrons la méthode des Moindres Carrés Ordinaires (MCO) en utilisant une approche matricielle. Cette méthode est particulièrement utile pour résoudre les problèmes de régression avec plusieurs variables explicatives (régression multiple), où les calculs matriciels permettent de simplifier le processus de minimisation.

L'objectif est de trouver les paramètres \(\beta = (\beta_1, \beta_2, \dots, \beta_k)\) du modèle de régression linéaire qui minimisent la somme des carrés des erreurs ou des résidus.

\section{Modèle de Régression Linéaire}
Le modèle de régression linéaire univarié est donné par :
\[
Y_i = \beta_1 + \beta_2 X_i + \mu_i
\]
où :
\begin{itemize}
    \item \(Y_i\) est la variable dépendante pour l'observation \(i\),
    \item \(X_i\) est la variable indépendante (explanatoire) pour l'observation \(i\),
    \item \(\beta_1\) et \(\beta_2\) sont les paramètres du modèle (intercept et pente) à estimer,
    \item \(\mu_i\) est le terme d'erreur pour l'observation \(i\).
\end{itemize}

Nous pouvons exprimer ce modèle de manière matricielle pour une régression multiple (c'est-à-dire avec plusieurs variables explicatives) sous la forme suivante :
\[
Y = X\beta + u
\]
où :
\begin{itemize}
    \item \(Y\) est un vecteur colonne \(n \times 1\) des observations de la variable dépendante,
    \item \(X\) est une matrice \(n \times (k+1)\) des valeurs des variables explicatives, où chaque ligne contient un ensemble de variables explicatives, avec une colonne de 1 pour le terme d'intercept,
    \item \(\beta\) est un vecteur colonne \( (k+1) \times 1 \) des coefficients à estimer \(\beta_0, \beta_1, \dots, \beta_k\),
    \item \(u\) est un vecteur \(n \times 1\) des erreurs ou résidus (\(u \sim N(0, \sigma^2I)\)).
\end{itemize}

\textbf{Hypothèses sur les erreurs} :
\begin{itemize}
    \item \(E(u) = 0\), les erreurs ont une moyenne nulle,
    \item \(V(u) = \sigma^2I\), les erreurs ont une variance constante \(\sigma^2\) et sont indépendantes entre elles.
\end{itemize}

\section{Sum of Squared Residuals}
L'objectif des MCO est de minimiser la somme des carrés des résidus, qui est une mesure de l'écart total entre les valeurs observées et les valeurs prédites par le modèle. Sous forme matricielle, cette fonction de cote est donnée par :
\[
S = u^\top u = (Y - X\beta)^\top (Y - X\beta)
\]
Nous cherchons à minimiser \(S\), la somme des carrés des résidus. Cela revient à minimiser l'écart entre les valeurs observées \(Y\) et les valeurs prédites \(X\beta\).

Développons cette fonction :
\[
S = (Y - X\beta)^\top (Y - X\beta) = Y^\top Y - Y^\top X\beta - \beta^\top X^\top Y + \beta^\top X^\top X\beta
\]
Notez que \(Y^\top X\beta\) est un scalaire, donc il est égal à son transpose \( \beta^\top X^\top Y\). Ainsi, nous avons :
\[
S = Y^\top Y - 2\beta^\top X^\top Y + \beta^\top X^\top X\beta
\]
Nous cherchons les valeurs de \(\beta\) qui minimisent cette fonction.

\section{Minimisation de la Fonction du Sum of Squared Residuals}
Pour trouver les valeurs optimales de \(\beta\), nous devons dériver la fonction de somme des carrés des résidus \(S\) par rapport à \(\beta\) et résoudre pour \(\beta\).

\subsection{Calcul de la Dérivée de \(S\) par Rapport à \(\beta\)}
Nous procédons à la dérivation de \(S\) par rapport à \(\beta\). La dérivée de chaque terme de \(S\) est calculée comme suit :
\[
\frac{\partial S}{\partial \beta} = -2X^\top Y + 2X^\top X\beta
\]
Cette dérivée nous indique comment la fonction \(S\) change en fonction des paramètres \(\beta\). Nous égalons cette dérivée à zéro pour trouver le minimum :
\[
-2X^\top Y + 2X^\top X\beta = 0
\]
Simplifions cette équation :
\[
X^\top X\beta = X^\top Y
\]
Cela nous donne les équations normales, qui sont les conditions nécessaires pour que \(S\) soit minimisé.

\subsection{Solution des Équations Normales}
Les équations normales sont :
\[
X^\top X\beta = X^\top Y
\]
Pour résoudre pour \(\beta\), nous isolons \(\beta\) :
\[
\beta = (X^\top X)^{-1} X^\top Y
\]
Cela donne l'estimateur des Moindres Carrés Ordinaires pour les paramètres \(\beta\). Cette solution exprime les coefficients estimés comme une combinaison linéaire des variables explicatives \(X\), pondérées par les valeurs observées \(Y\).

\section{Conclusion}
Les Moindres Carrés Ordinaires offrent une méthode efficace pour estimer les paramètres d'un modèle de régression linéaire. En utilisant la matrice \(X\), le vecteur \(Y\), et les équations normales, nous pouvons obtenir les estimations des paramètres \(\beta\) qui minimisent la somme des carrés des résidus. La solution matricielle est donnée par :
\[
\hat{\beta} = (X^\top X)^{-1} X^\top Y
\]
Cet estimateur est largement utilisé en économétrie et dans d'autres domaines pour ajuster des modèles linéaires aux données.

\end{document}