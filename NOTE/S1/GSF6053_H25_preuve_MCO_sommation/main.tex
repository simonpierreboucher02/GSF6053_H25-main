\documentclass[14pt]{extarticle} % Utilisation de extarticle pour supporter 14pt

% ------------------------------------------------------------------------
% Packages indispensables et recommandés
% ------------------------------------------------------------------------
\usepackage[utf8]{inputenc}     % Encodage des caractères
\usepackage[T1]{fontenc}        % Encodage de la police
\usepackage[french]{babel}      % Support de la langue française
\usepackage{amsmath, amssymb}   % Environnement mathématique enrichi
\usepackage{graphicx}           % Inclusion d'images
\usepackage{hyperref}           % Liens hypertextes
\usepackage{geometry}           % Gestion des marges
\usepackage{titlesec}           % Personnalisation des titres
\usepackage{setspace}           % Gestion de l'interligne
\usepackage{xcolor}             % Gestion des couleurs
\usepackage{lipsum}             % (optionnel) Pour générer du faux texte
\usepackage{booktabs}           % (optionnel) Pour améliorer les tableaux
\usepackage{fancyhdr}           % Personnalisation des en-têtes et pieds de page
\usepackage{cleveref}           % Références intelligentes
\usepackage{caption}            % Pour personnaliser les légendes

% ------------------------------------------------------------------------
% Configuration de la mise en page
% ------------------------------------------------------------------------
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm
}

% ------------------------------------------------------------------------
% Personnalisation des sections
% ------------------------------------------------------------------------
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ------------------------------------------------------------------------
% Commande pour mettre en avant les livres en bleu
% ------------------------------------------------------------------------
\newcommand{\livre}[1]{\textcolor{blue}{#1}}

% ------------------------------------------------------------------------
% Configuration des hyperliens
% ------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,          % Les liens sont colorés
    linkcolor=blue,           % Couleur des liens internes (TOC, références, etc.)
    urlcolor=blue,            % Couleur des URL
    citecolor=blue,           % Couleur des citations
    filecolor=blue,           % Couleur des liens vers des fichiers
    pdfborder={0 0 0},        % Pas de bordure autour des liens
    breaklinks=true            % Permet les sauts de ligne dans les liens
}

% ------------------------------------------------------------------------
% Configuration des en-têtes et pieds de page
% ------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Preuve des MCO en Sommation}
\fancyhead[R]{Hiver 2025}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\includegraphics[height=1cm]{logo_universite_laval.png}} % Ajout du logo ERS en bas à droite

\title{Preuve des Moindres Carrés Ordinaires (MCO) en Sommation}
\author{Simon-Pierre Boucher}
\date{Janvier 2025}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
Les Moindres Carrés Ordinaires (MCO) sont une méthode d'estimation utilisée pour ajuster un modèle de régression linéaire aux données observées en minimisant l'erreur quadratique entre les valeurs observées et les valeurs prédites. L'objectif ici est de démontrer les MCO en utilisant la méthode de minimisation de la somme des carrés des résidus, en suivant une procédure rigoureuse étape par étape.

\section{Modèle de Régression Linéaire}
Le modèle de régression linéaire simple peut être écrit comme suit :
\[
Y_i = \beta_1 + \beta_2 X_i + \mu_i
\]
où :
\begin{itemize}
    \item \(Y_i\) est la variable dépendante ou expliquée pour l'observation \(i\),
    \item \(X_i\) est la variable indépendante ou explicative pour l'observation \(i\),
    \item \(\beta_1\) et \(\beta_2\) sont les paramètres du modèle à estimer,
    \item \(\mu_i\) est le terme d'erreur associé à l'observation \(i\).
\end{itemize}

\textbf{Hypothèses} :
\begin{itemize}
    \item \(E(\mu_i) = 0\), ce qui signifie que les erreurs ont une moyenne nulle,
    \item \(V(\mu_i) = \sigma^2\), c'est-à-dire que les erreurs ont une variance constante,
    \item Les erreurs sont indépendantes les unes des autres.
\end{itemize}

Nous pouvons réécrire ce modèle comme suit :
\[
Y_i = \beta_1 + \beta_2 X_i + \mu_i
\]

\section{Les Résidus}
Les résidus sont la différence entre les valeurs observées \(Y_i\) et les valeurs estimées \(\hat{Y}_i\) par le modèle de régression. L'estimateur des résidus est donc donné par :
\[
\hat{\mu}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_1 + \hat{\beta}_2 X_i)
\]
L'objectif des MCO est de minimiser la somme des carrés des résidus. Cela revient à minimiser la fonction :
\[
S = \sum_{i=1}^N \hat{\mu}_i^2 = \sum_{i=1}^N (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i)^2
\]
où \(N\) est le nombre d'observations. Cette somme représente l'erreur totale entre les valeurs observées et les valeurs prédites.

\section{Minimisation de la Somme des Carrés des Résidus}
L'objectif de la méthode des Moindres Carrés Ordinaires est de trouver les valeurs des paramètres \(\hat{\beta}_1\) et \(\hat{\beta}_2\) qui minimisent cette somme. Nous cherchons donc à résoudre le problème suivant :
\[
\min_{\hat{\beta}_1, \hat{\beta}_2} \sum_{i=1}^N (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i)^2
\]
Cela revient à dériver la fonction \(S\) par rapport aux paramètres \(\hat{\beta}_1\) et \(\hat{\beta}_2\), et de les égaler à zéro.

\section{Calcul des Équations Normales}
Nous obtenons les équations normales en dérivant \(S\) par rapport à \(\hat{\beta}_1\) et \(\hat{\beta}_2\).

\subsection{Équation Normale pour \(\hat{\beta}_1\)}
La dérivée partielle de \(S\) par rapport à \(\hat{\beta}_1\) est :
\[
\frac{\partial}{\partial \hat{\beta}_1} \sum_{i=1}^N (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i)^2 = -2 \sum_{i=1}^N (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i) = 0
\]
Cette dérivée mesure comment \(S\) varie lorsque \(\hat{\beta}_1\) varie, et la condition d'optimalité exige que cette dérivée soit égale à zéro. En réorganisant cette équation, nous obtenons :
\[
\sum_{i=1}^N Y_i = N \hat{\beta}_1 + \hat{\beta}_2 \sum_{i=1}^N X_i
\]
Isolons \(\hat{\beta}_1\) :
\[
\hat{\beta}_1 = \frac{1}{N} \sum_{i=1}^N Y_i - \hat{\beta}_2 \frac{1}{N} \sum_{i=1}^N X_i
\]
Ce qui peut se réécrire comme :
\[
\hat{\beta}_1 = \bar{Y} - \hat{\beta}_2 \bar{X}
\]
où \(\bar{Y}\) et \(\bar{X}\) représentent les moyennes des \(Y_i\) et \(X_i\) respectivement.

\subsection{Équation Normale pour \(\hat{\beta}_2\)}
La dérivée partielle de \(S\) par rapport à \(\hat{\beta}_2\) est :
\[
\frac{\partial}{\partial \hat{\beta}_2} \sum_{i=1}^N (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i)^2 = -2 \sum_{i=1}^N X_i (Y_i - \hat{\beta}_1 - \hat{\beta}_2 X_i) = 0
\]
Cette dérivée mesure comment \(S\) varie lorsque \(\hat{\beta}_2\) varie. En réorganisant cette équation, nous obtenons l'équation normale suivante :
\[
\sum_{i=1}^N X_i Y_i - \hat{\beta}_1 \sum_{i=1}^N X_i - \hat{\beta}_2 \sum_{i=1}^N X_i^2 = 0
\]
En utilisant la relation \(\hat{\beta}_1 = \bar{Y} - \hat{\beta}_2 \bar{X}\), nous remplaçons dans cette équation pour obtenir :
\[
\hat{\beta}_2 = \frac{\sum_{i=1}^N (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^N (X_i - \bar{X})^2}
\]
C'est l'estimateur de \(\hat{\beta}_2\), qui est l'estimateur de la pente de la droite de régression.

\section{Conclusion}
Les estimateurs des paramètres de régression \(\beta_1\) et \(\beta_2\) obtenus par la méthode des Moindres Carrés Ordinaires sont :
\[
\hat{\beta}_1 = \bar{Y} - \hat{\beta}_2 \bar{X}
\]
et
\[
\hat{\beta}_2 = \frac{\sum_{i=1}^N (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^N (X_i - \bar{X})^2}
\]
Ces estimations minimisent la somme des carrés des résidus et permettent de déterminer la relation linéaire entre \(Y\) et \(X\). Ces résultats sont fondamentaux pour l'application des Moindres Carrés Ordinaires dans l'analyse de régression linéaire.

\end{document}