\documentclass[14pt]{extarticle} % Utilisation de extarticle pour supporter 14pt

% ------------------------------------------------------------------------
% Packages indispensables et recommandés
% ------------------------------------------------------------------------
\usepackage[utf8]{inputenc}     % Encodage des caractères
\usepackage[T1]{fontenc}        % Encodage de la police
\usepackage[french]{babel}      % Support de la langue française
\usepackage{amsmath, amssymb}   % Environnements mathématiques enrichis
\usepackage{amsthm}             % Environnements théorèmes
\usepackage{graphicx}           % Inclusion d'images
\usepackage{hyperref}           % Liens hypertextes
\usepackage{geometry}           % Gestion des marges
\usepackage{titlesec}           % Personnalisation des titres
\usepackage{setspace}           % Gestion de l'interligne
\usepackage{xcolor}             % Gestion des couleurs
\usepackage{booktabs}           % Pour améliorer les tableaux
\usepackage{fancyhdr}           % Personnalisation des en-têtes et pieds de page
\usepackage{cleveref}           % Références intelligentes
\usepackage{caption}            % Pour personnaliser les légendes
\usepackage{enumitem}           % Pour personnaliser les listes
\usepackage{pdflscape}          % Pour placer les tableaux en paysage

% ------------------------------------------------------------------------
% Configuration de la mise en page
% ------------------------------------------------------------------------
\geometry{
    a4paper,
    margin=25mm
}

% ------------------------------------------------------------------------
% Définition des environnements théorèmes, définitions, etc.
% ------------------------------------------------------------------------
\theoremstyle{definition}
\newtheorem{definition}{Définition}[section]

\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Théorème}[section]

% ------------------------------------------------------------------------
% Personnalisation des sections
% ------------------------------------------------------------------------
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ------------------------------------------------------------------------
% Commande pour mettre en avant les livres en bleu
% ------------------------------------------------------------------------
\newcommand{\livre}[1]{\textcolor{blue}{#1}}

% ------------------------------------------------------------------------
% Configuration des hyperliens
% ------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,          % Les liens sont colorés
    linkcolor=blue,           % Couleur des liens internes (TOC, références, etc.)
    urlcolor=blue,            % Couleur des URL
    citecolor=blue,           % Couleur des citations
    filecolor=blue,           % Couleur des liens vers des fichiers
    pdfborder={0 0 0},        % Pas de bordure autour des liens
    breaklinks=true            % Permet les sauts de ligne dans les liens
}
% ------------------------------------------------------------------------
% Configuration des en-têtes et pieds de page
% ------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Preuve Détaillée des Moindres Carrés Généralisés (GLS)}
\fancyhead[R]{Hiver 2025}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\includegraphics[height=1cm]{logo_universite_laval.png}} % Ajout du logo ERS en bas à droite


% ------------------------------------------------------------------------
% Informations sur le document
% ------------------------------------------------------------------------
\title{\Huge Preuve Détaillée des Moindres Carrés Généralisés (GLS)}
\author{GSF-6053}
\date{Hiver 2025}

% ------------------------------------------------------------------------
% Début du document
% ------------------------------------------------------------------------
\begin{document}

\maketitle

\tableofcontents
\newpage

% Augmenter l'interligne à 1,5
\onehalfspacing

% ------------------------------------------------------------------------
% SECTION : Preuve Détaillée des Moindres Carrés Généralisés
% ------------------------------------------------------------------------


Les Moindres Carrés Généralisés (GLS) sont une extension des Moindres Carrés Ordinaires (OLS) qui permettent de traiter des situations où les hypothèses de base des OLS, notamment l'homoscédasticité et l'absence d'autocorrélation des erreurs, sont violées. Cette section fournit une preuve détaillée de la méthode GLS, en démontrant sa dérivation, ses propriétés et son optimalité.

\section{Modèle Linéaire Généralisé}

Considérons le modèle linéaire suivant :
\begin{equation}
    Y = X\beta + u
\end{equation}
où :
\begin{itemize}
    \item \( Y \) est un vecteur \( T \times 1 \) des variables dépendantes.
    \item \( X \) est une matrice \( T \times K \) des variables explicatives.
    \item \( \beta \) est un vecteur \( K \times 1 \) des paramètres à estimer.
    \item \( u \) est un vecteur \( T \times 1 \) des termes d'erreur.
\end{itemize}

\subsection{Hypothèses du Modèle}

Pour les GLS, nous relaxons certaines hypothèses des OLS. Les hypothèses principales sont :
\begin{itemize}
    \item \( E(u) = 0 \) : Les erreurs ont une espérance nulle.
    \item \( E(uu') = V = \sigma^2 \Omega \) : La matrice de variance-covariance des erreurs est proportionnelle à une matrice \( \Omega \) qui n'est pas nécessairement diagonale.
\end{itemize}

\section{Dérivation des Estimateurs GLS}

L'objectif des GLS est d'obtenir un estimateur \(\hat{\beta}_{GLS}\) qui minimise une forme quadratique pondérée des résidus, prenant en compte la structure de la matrice \( \Omega \).

\subsection{Transformation du Modèle}

Pour simplifier le problème, nous transformons le modèle en multipliant par \( \Omega^{-1/2} \), où \( \Omega^{-1/2} \) est une matrice telle que :
\[
\Omega^{-1/2} \Omega \Omega^{-1/2}' = I
\]
Cela suppose que \( \Omega \) est symétrique et positive définie, garantissant l'existence de \( \Omega^{-1/2} \).

Le modèle transformé devient :
\[
\Omega^{-1/2} Y = \Omega^{-1/2} X \beta + \Omega^{-1/2} u
\]
Notons :
\[
Y^* = \Omega^{-1/2} Y, \quad X^* = \Omega^{-1/2} X, \quad u^* = \Omega^{-1/2} u
\]
Ainsi, le modèle transformé s'écrit :
\begin{equation}
    Y^* = X^* \beta + u^*
\end{equation}

\subsection{Propriétés des Erreurs Transformées}

Nous examinons les propriétés des termes d'erreur \( u^* \) :
\[
E(u^*) = \Omega^{-1/2} E(u) = 0
\]
\[
E(u^* u^{*'}) = \Omega^{-1/2} E(u u') \Omega^{-1/2}' = \Omega^{-1/2} (\sigma^2 \Omega) \Omega^{-1/2}' = \sigma^2 I
\]
Ainsi, le modèle transformé satisfait les hypothèses des OLS :
\begin{itemize}
    \item Espérance nulle des erreurs.
    \item Variance-covariance des erreurs égale à \( \sigma^2 I \) (homoscédasticité et absence d'autocorrélation).
\end{itemize}

\subsection{Application des Moindres Carrés Ordinaires au Modèle Transformé}

Appliquons les OLS au modèle transformé :
\[
\hat{\beta}_{GLS} = (X^{*'} X^*)^{-1} X^{*'} Y^*
\]
En remplaçant \( X^* \) et \( Y^* \) :
\[
\hat{\beta}_{GLS} = (\Omega^{-1/2} X)' (\Omega^{-1/2} X)^{-1} (\Omega^{-1/2} X)' (\Omega^{-1/2} Y)
\]
\[
\hat{\beta}_{GLS} = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} Y
\]
Ceci est l'estimateur des Moindres Carrés Généralisés.

\section{Propriétés de l'Estimateur GLS}

\subsection{Sans Biais}

Pour démontrer que \(\hat{\beta}_{GLS}\) est sans biais :
\[
E(\hat{\beta}_{GLS}) = E\left[ (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} Y \right]
\]
En remplaçant \( Y = X\beta + u \) :
\[
E(\hat{\beta}_{GLS}) = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} (X\beta + u)
\]
\[
E(\hat{\beta}_{GLS}) = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} X \beta + (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} E(u)
\]
Comme \( E(u) = 0 \) :
\[
E(\hat{\beta}_{GLS}) = \beta
\]
Ainsi, l'estimateur GLS est sans biais.

\subsection{Variance de l'Estimateur GLS}

Calculons la variance de \(\hat{\beta}_{GLS}\) :
\[
\text{Var}(\hat{\beta}_{GLS}) = \text{Var}\left[ (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} Y \right]
\]
Remplaçons \( Y = X\beta + u \) :
\[
\text{Var}(\hat{\beta}_{GLS}) = \text{Var}\left[ (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} u \right]
\]
Puisque \( \text{Var}(u) = \sigma^2 \Omega \) :
\[
\text{Var}(\hat{\beta}_{GLS}) = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} \text{Var}(u) \Omega^{-1} X (X' \Omega^{-1} X)^{-1}
\]
\[
\text{Var}(\hat{\beta}_{GLS}) = \sigma^2 (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} \Omega \Omega^{-1} X (X' \Omega^{-1} X)^{-1}
\]
\[
\text{Var}(\hat{\beta}_{GLS}) = \sigma^2 (X' \Omega^{-1} X)^{-1}
\]
Ainsi, la variance de l'estimateur GLS est donnée par :
\[
\text{Var}(\hat{\beta}_{GLS}) = \sigma^2 (X' \Omega^{-1} X)^{-1}
\]

\subsection{Optimalité de l'Estimateur GLS}

L'estimateur GLS est BLUE (Best Linear Unbiased Estimator), c'est-à-dire qu'il est le meilleur estimateur linéaire sans biais en termes de variance parmi tous les estimateurs linéaires sans biais. La preuve repose sur le théorème de Gauss-Markov généralisé.

\paragraph{Théorème de Gauss-Markov Généralisé}

Sous les hypothèses suivantes :
\begin{itemize}
    \item \( E(u) = 0 \)
    \item \( E(uu') = \sigma^2 \Omega \)
    \item \( \Omega \) est connue et positive définie
\end{itemize}
L'estimateur GLS est le meilleur estimateur linéaire sans biais, c'est-à-dire qu'il minimise la variance parmi tous les estimateurs linéaires sans biais.

\paragraph{Preuve de l'Optimalité}

Soit \(\tilde{\beta}\) un estimateur linéaire sans biais de \(\beta\), tel que :
\[
\tilde{\beta} = C Y
\]
où \( C \) est une matrice \( K \times T \).

L'estimateur est sans biais si :
\[
E(\tilde{\beta}) = C X \beta = \beta \quad \Rightarrow \quad C X = I_K
\]
La variance de \(\tilde{\beta}\) est :
\[
\text{Var}(\tilde{\beta}) = C \text{Var}(Y) C' = C \sigma^2 \Omega C'
\]
Nous souhaitons minimiser \(\text{Var}(\tilde{\beta})\) sous la contrainte \( C X = I_K \).

Utilisons la méthode des multiplicateurs de Lagrange. Définissons la fonction de Lagrange :
\[
\mathcal{L} = \text{tr}(C \Omega C') - \text{tr}(\Lambda (C X - I))
\]
où \( \Lambda \) est une matrice des multiplicateurs de Lagrange.

En prenant les dérivées partielles par rapport à \( C \) et en les égalant à zéro, nous obtenons :
\[
\frac{\partial \mathcal{L}}{\partial C} = 2 C \Omega - \Lambda X' = 0 \quad \Rightarrow \quad C \Omega = \frac{1}{2} \Lambda X'
\]
Cependant, pour minimiser la variance, il est plus approprié de travailler avec la minimisation quadratique sous contrainte linéaire. En résolvant ce problème, on trouve que :
\[
C = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1}
\]
ce qui correspond exactement à l'estimateur GLS.

Ainsi, \(\hat{\beta}_{GLS} = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} Y\) est l'estimateur linéaire sans biais avec la plus petite variance, prouvant qu'il est BLUE.

\section{Estimateur Faisable des GLS (FGLS)}

Dans la pratique, la matrice \( \Omega \) est rarement connue. Par conséquent, nous devons estimer \( \Omega \), ce qui conduit à l'estimateur des Moindres Carrés Généralisés Faisables (FGLS).

\subsection{Étapes de l'Estimateur FGLS}

\begin{enumerate}
    \item \textbf{Estimation Initiale avec OLS} : Estimez le modèle en utilisant les OLS pour obtenir un premier estimateur \(\hat{\beta}_{OLS}\).
    \[
    \hat{\beta}_{OLS} = (X'X)^{-1} X' Y
    \]
    
    \item \textbf{Estimation des Résidus} : Calculez les résidus de la première estimation.
    \[
    \hat{u} = Y - X \hat{\beta}_{OLS}
    \]
    
    \item \textbf{Estimation de \( \Omega \)} : Utilisez les résidus \(\hat{u}\) pour estimer la matrice \( \Omega \). Cette étape dépend de la structure présumée de \( \Omega \). Par exemple, si nous supposons une hétéroscédasticité où \( \text{Var}(u_i) = \sigma^2 \omega_{ii} \), une approche pourrait être d'estimer \(\omega_{ii}\) comme une fonction des variables explicatives.
    
    \item \textbf{Transformation du Modèle} : Transformez le modèle en utilisant l'estimation \(\hat{\Omega}^{-1/2}\).
    \[
    \hat{\Omega}^{-1/2} Y = \hat{\Omega}^{-1/2} X \beta + \hat{\Omega}^{-1/2} u
    \]
    
    \item \textbf{Estimation Finale avec OLS} : Appliquez les OLS au modèle transformé pour obtenir l'estimateur FGLS.
    \[
    \hat{\beta}_{FGLS} = (X' \hat{\Omega}^{-1} X)^{-1} X' \hat{\Omega}^{-1} Y
    \]
\end{enumerate}

\subsection{Propriétés de l'Estimateur FGLS}

Lorsque \( \hat{\Omega} \) est une estimation consistante de \( \Omega \), l'estimateur FGLS est :
\begin{itemize}
    \item \textbf{Sans biais} : Similaire à GLS, FGLS est sans biais.
    \item \textbf{Consistant} : \(\hat{\beta}_{FGLS}\) converge en probabilité vers \(\beta\) lorsque la taille de l'échantillon tend vers l'infini.
    \item \textbf{Asymptotiquement efficace} : Sous certaines conditions, FGLS atteint l'efficacité asymptotique, c'est-à-dire qu'il a la plus petite variance asymptotique parmi les estimateurs linéaires sans biais.
\end{itemize}

\section{Comparaison entre OLS et GLS}

\subsection{Variance des Estimateurs}

Pour comparer OLS et GLS, examinons les variances de leurs estimateurs respectifs :
\begin{itemize}
    \item \textbf{OLS} :
    \[
    \text{Var}(\hat{\beta}_{OLS}) = \sigma^2 (X'X)^{-1}
    \]
    \item \textbf{GLS} :
    \[
    \text{Var}(\hat{\beta}_{GLS}) = \sigma^2 (X' \Omega^{-1} X)^{-1}
    \]
\end{itemize}

\paragraph{Comparaison de la Variance}

Supposons que \( \Omega = I \), alors \( \hat{\beta}_{GLS} = \hat{\beta}_{OLS} \) et les variances sont identiques. Lorsque \( \Omega \neq I \), GLS ajuste pour la structure de la variance-covariance des erreurs, généralement réduisant la variance des estimateurs comparé à OLS.

\subsection{Conditions d'Optimalité}

GLS est plus efficace que OLS lorsque :
\begin{itemize}
    \item Les erreurs présentent une hétéroscédasticité ou une autocorrélation.
    \item La matrice \( \Omega \) est correctement spécifiée.
\end{itemize}
Dans ce cas, GLS fournit des estimateurs avec une variance inférieure à celle des OLS, rendant les inférences plus fiables.

\section{Conclusion}

Les Moindres Carrés Généralisés offrent une méthode puissante pour estimer les paramètres d'un modèle linéaire lorsque les hypothèses de base des OLS sont violées, notamment en présence d'hétéroscédasticité ou d'autocorrélation des erreurs. La preuve détaillée présentée ici démontre que, sous les conditions appropriées, GLS est un estimateur sans biais et plus efficace que les OLS. Cependant, la faisabilité de GLS dépend de la capacité à estimer correctement la matrice \( \Omega \), ce qui est souvent réalisé à travers l'estimateur FGLS.

\end{document}
