\documentclass[14pt]{extarticle} % Utilisation de extarticle pour supporter 14pt

% ------------------------------------------------------------------------
% Packages indispensables et recommandés
% ------------------------------------------------------------------------
\usepackage[utf8]{inputenc}     % Encodage des caractères
\usepackage[T1]{fontenc}        % Encodage de la police
\usepackage[french]{babel}      % Support de la langue française
\usepackage{amsmath, amssymb}   % Environnement mathématique enrichi
\usepackage{graphicx}           % Inclusion d'images
\usepackage{hyperref}           % Liens hypertextes
\usepackage{geometry}           % Gestion des marges
\usepackage{titlesec}           % Personnalisation des titres
\usepackage{setspace}           % Gestion de l'interligne
\usepackage{xcolor}             % Gestion des couleurs
\usepackage{lipsum}             % (optionnel) Pour générer du faux texte
\usepackage{booktabs}           % (optionnel) Pour améliorer les tableaux
\usepackage{fancyhdr}           % Personnalisation des en-têtes et pieds de page
\usepackage{cleveref}           % Références intelligentes
\usepackage{caption}            % Pour personnaliser les légendes

% ------------------------------------------------------------------------
% Configuration de la mise en page
% ------------------------------------------------------------------------
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm
}

% ------------------------------------------------------------------------
% Personnalisation des sections
% ------------------------------------------------------------------------
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ------------------------------------------------------------------------
% Commande pour mettre en avant les livres en bleu
% ------------------------------------------------------------------------
\newcommand{\livre}[1]{\textcolor{blue}{#1}}

% ------------------------------------------------------------------------
% Configuration des hyperliens
% ------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,          % Les liens sont colorés
    linkcolor=blue,           % Couleur des liens internes (TOC, références, etc.)
    urlcolor=blue,            % Couleur des URL
    citecolor=blue,           % Couleur des citations
    filecolor=blue,           % Couleur des liens vers des fichiers
    pdfborder={0 0 0},        % Pas de bordure autour des liens
    breaklinks=true            % Permet les sauts de ligne dans les liens
}

% ------------------------------------------------------------------------
% Configuration des en-têtes et pieds de page
% ------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Le Maximum de Vraisemblance}
\fancyhead[R]{Hiver 2025}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\includegraphics[height=1cm]{logo_universite_laval.png}} % Ajout du logo ERS en bas à droite

% ------------------------------------------------------------------------
% Informations sur le document
% ------------------------------------------------------------------------
\title{\textbf{Le Maximum de Vraisemblance}}
\author{GSF-6053}
\date{Hiver 2025}

% ------------------------------------------------------------------------
% Autoriser les coupures d'équations
% ------------------------------------------------------------------------
\allowdisplaybreaks

% ------------------------------------------------------------------------
% Début du document
% ------------------------------------------------------------------------
\begin{document}

\maketitle

\tableofcontents
\newpage

% Augmenter l'interligne à 1,5
\onehalfspacing

% ------------------------------------------------------------------------
% SECTION 1 : Introduction aux Moindres Carrés Ordinaires (MCO)
% ------------------------------------------------------------------------
\section{Introduction aux Moindres Carrés Ordinaires (MCO)}

\textbf{Chapitres 3 de \livre{Gujarati et Porter}, Chapitre 2 de \livre{Wooldridge}.}

Les Moindres Carrés Ordinaires (MCO) sont une méthode d'estimation utilisée en économétrie pour déterminer les paramètres d'un modèle de régression linéaire. L'objectif principal des MCO est de minimiser la somme des carrés des résidus, c'est-à-dire la somme des différences au carré entre les valeurs observées et les valeurs prédites par le modèle.

\subsection{Objectif des MCO}

L'objectif des MCO est d'estimer les paramètres \(\beta\) dans le modèle de régression linéaire suivant :
\begin{equation*}
Y = X\beta + u,
\end{equation*}
où :
\begin{itemize}
    \item \(Y\) est le vecteur des variables dépendantes.
    \item \(X\) est la matrice des variables explicatives (incluant une colonne de constantes si nécessaire).
    \item \(\beta\) est le vecteur des paramètres à estimer.
    \item \(u\) est le vecteur des termes d'erreur.
\end{itemize}

\subsection{Hypothèses des MCO}

Pour que les estimateurs des MCO soient les meilleurs estimateurs linéaires sans biais (BLUE - Best Linear Unbiased Estimators), plusieurs hypothèses doivent être respectées :
\begin{enumerate}
    \item \textbf{Linéarité des Paramètres} : Le modèle doit être linéaire en ses paramètres \(\beta\).
    \item \textbf{Exogénéité des Régresseurs} : Les variables explicatives \(X\) doivent être exogènes, c'est-à-dire que \(E(u \mid X) = 0\).
    \item \textbf{Absence de Multicolinéarité Parfaite} : Les variables explicatives ne doivent pas être parfaitement corrélées entre elles.
    \item \textbf{Homoscedasticité} : La variance des termes d'erreur \(u\) doit être constante, \(\text{Var}(u \mid X) = \sigma^2 I\).
    \item \textbf{Indépendance des Termes d'Erreur} : Les termes d'erreur \(u\) doivent être indépendants entre eux.
\end{enumerate}

\subsection{Propriétés des Estimateurs MCO}

Sous les hypothèses précédentes, les estimateurs des MCO \(\hat{\beta}\) possèdent les propriétés suivantes :
\begin{itemize}
    \item \textbf{Non Biaisés} : \(E(\hat{\beta}) = \beta\).
    \item \textbf{Minimum de la Variance} : Parmi tous les estimateurs linéaires non biaisés, \(\hat{\beta}\) a la variance la plus faible.
    \item \textbf{Consistance} : Lorsque la taille de l'échantillon augmente, \(\hat{\beta}\) converge en probabilité vers \(\beta\).
    \item \textbf{Normalité} : Si les termes d'erreur \(u\) sont normalement distribués, alors \(\hat{\beta}\) suit une distribution normale.
\end{itemize}

\subsection{Importance des MCO en Econométrie}

Les MCO sont fondamentaux en économétrie car ils fournissent une méthode simple et efficace pour estimer les relations entre les variables économiques. Ils sont utilisés dans une multitude d'applications, telles que :
\begin{itemize}
    \item Estimation de l'impact des facteurs économiques sur la consommation.
    \item Analyse de la relation entre l'investissement et le taux d'intérêt.
    \item Étude de l'effet des dépenses publicitaires sur les ventes.
\end{itemize}

Les MCO servent également de base pour des méthodes plus avancées en économétrie, telles que la régression avec variables endogènes, les modèles à effets fixes, et les modèles de panel.

% ------------------------------------------------------------------------
% SECTION 2 : Le Maximum de Vraisemblance
% ------------------------------------------------------------------------
\section{Le Maximum de Vraisemblance}

\subsection{Introduction}

\livre{Gujarati et Porter} : Appendice 4.A et chapitre 7 (appendice 7.A), Appendice A à partir de la page 825.

\livre{Wooldridge} appendice C C-4b.

Référence supplémentaire si nécessaire : Chapitre 14 dans le livre de \livre{Greene}. En particulier 14.9.1 pour le cas du modèle de régression linéaire simple.

Soit \(\theta\) le vecteur de paramètres à estimer. Le principe est le suivant :

Donner à \(\theta\) la valeur qui maximise la probabilité d’obtenir un échantillon identique à celui qu’on dispose.

En détail, le principe est le suivant :

\begin{itemize}
    \item La méthode du maximum de vraisemblance est une technique d’estimation des paramètres d’un modèle (d’une équation ou d’un système, linéaire ou non linéaire) avec ou sans restrictions sur les paramètres (coefficients, matrice de variances et covariances).
    \item Elle nécessite que la loi de distribution des observations soit connue (à des paramètres inconnus près).
    \item On construit une fonction des observations et des paramètres appelée fonction de vraisemblance construite à partir de la fonction de densité.
    \item On cherche les valeurs des paramètres qui maximisent cette fonction, étant données les observations dont on dispose.
    \item La valeur proposée comme estimateur des paramètres est celle qui rend le plus probable le fait que les observations que l’on a soient des réalisations de la loi de distribution ; cette valeur est appelée estimateur du maximum de vraisemblance (MLE).
\end{itemize}

Pour comprendre intuitivement l’estimateur de vraisemblance, regardons au tableau un exemple avec la distribution de Poisson en page 511 de \livre{Greene} :

\subsection{Exemple : Modèle de Régression Linéaire}

\subsubsection{La Vraisemblance Normale}

Comme mentionné plus haut, le MLE nécessite de faire une hypothèse distributionnelle sur les \(u_t\).

Sous l’hypothèse

\begin{equation*}
u_t \sim \text{i.i.d.} \ N(0 , \sigma^2)
\end{equation*}

La loi conjointe des \(u_1, u_2, \ldots, u_T\) est le produit des normales de moyenne 0 et variance \(\sigma^2\). Ainsi, par l’hypothèse d’indépendance, la loi conjointe des \(u_t\) est égale au produit des densités des \(u_t\) :

\begin{align*}
f(u_1, u_2, \ldots , u_T ) &= \prod_{t=1}^{T} f(u_t) \\
&= \left(\sqrt{2\pi\sigma^2}\right)^{-T} \exp\left\{-\frac{1}{2\sigma^2} \sum_{t=1}^{T} u_t^2 \right\}
\end{align*}

Nous sommes intéressés à la loi conjointe des \(Y_t\). Effectuer un changement de variable : \(u_t = Y_t - X'_t \beta\).

\footnote{Nous avons des hypothèses distributionnelles sur les erreurs, mais nous voulons écrire la vraisemblance des \(Y\). Il faut donc passer des erreurs au \(Y\) avec un changement de variable. Si \(u\) est une variable aléatoire continue avec une pdf \(f_x (x)\) et si \(y = g(x)\) est une fonction continue et monotone de \(x\).}

On peut écrire la loi conjointe des \(Y_t\) (la vraisemblance) :

\begin{align*}
g(Y_1, Y_2, \ldots , Y_T) &= \prod_{t=1}^{T} g(Y_t) \\
&= \left(\sqrt{2\pi\sigma^2}\right)^{-T} \exp\left\{-\frac{1}{2\sigma^2} \sum_{t=1}^{T} (Y_t - X'_t \beta)^2 \right\} \\
&= (2\pi\sigma^2)^{-T/2} \exp \left\{ -\frac{(Y - X\beta)'(Y - X\beta)}{2\sigma^2} \right\}
\end{align*}

Pour faciliter les calculs, on prend le logarithme de la fonction de vraisemblance. Elle transforme le produit en somme. Comme la transformation log est monotone, elle ne change pas le problème de maximisation. La log-vraisemblance (le log de la loi conjointe des \(Y_t\)) sera :

\begin{equation*}
\mathcal{L} = -T \ln(2\pi) - T \ln(\sigma^2) - \frac{(Y - X\beta)'(Y - X\beta)}{2\sigma^2}
\end{equation*}

On veut donc maximiser cette fonction de vraisemblance en fonction de \(\beta\) et \(\sigma^2\). Les conditions de premier ordre sont :

\begin{align*}
\frac{\partial \mathcal{L}}{\partial \beta} &= -\frac{1}{\sigma^2} \frac{\partial (Y'Y - 2Y'X\beta + \beta'X'X\beta)}{\partial \beta} = 0 \\
&\quad \Rightarrow \quad \hat{\beta} = (X'X)^{-1}X'Y
\end{align*}

\begin{equation*}
\frac{\partial \mathcal{L}}{\partial \sigma^2} = -\frac{T}{\sigma^2} + \frac{(Y - X\beta)'(Y - X\beta)}{\sigma^4} = 0 \quad \Rightarrow \quad \hat{\sigma}^2 = \frac{(Y - X\beta)'(Y - X\beta)}{T}
\end{equation*}

\textbf{Note :} L’estimateur ci-haut est l’estimateur du maximum de vraisemblance. On voudra maintenant étudier ces propriétés. Nous allons démontrer que cet estimateur est sans biais et efficace sous certaines conditions. Nous allons également voir comment corriger le biais de l'estimateur de \(\sigma^2\).

Le MLE de \(\sigma^2\) est biaisé vers le bas. Ce n’est pas une propriété désirable. Par contre, sur base de notre démonstration, nous pourrons trouver une alternative sans biais :

\begin{equation*}
\hat{s}^2 = \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{T - K}
\end{equation*}

C’est ce que nous étudions à présent.

\section{Propriété des Estimateurs du MLE et des MCO}

\textbf{Chapitres 3 de \livre{Gujarati et Porter}, Chapitre 3.5 et appendice D de \livre{Wooldridge}.}

On veut maintenant savoir si les estimateurs que nous avons trouvés sont bons. Il faut étudier le biais et la variance de l’estimateur.

Pour les MCO et l’estimateur de \(\hat{\beta}\) du MLE qui coïncident, le résultat connu est le théorème de Gauss-Markov :

\textbf{Théorème de Gauss-Markov :} Étant donné les hypothèses du modèle de régression classique, les MCO sont des estimateurs sans biais qui ont la variance minimale. On dit qu’ils sont BLUE : Best Linear Unbiased Estimator.

La démonstration de ce théorème est effectuée dans l’appendice 3.A de \livre{Gujarati et Porter}. Elle est aussi en section 3A.6 de \livre{Wooldridge}. Ici, la notation matricielle et les estimateurs du maximum de vraisemblance me permettent de faire la démonstration d’une autre manière.

\subsection{Définitions}

Notre objectif pour démontrer que les MCO sont BLUE et de trouver des estimateurs
\begin{enumerate}
    \item sans biais
    \item efficaces.
\end{enumerate}

\textbf{Sans biais :} L’espérance de l’estimateur égale à la vraie valeur du paramètre. (En moyenne, notre estimateur est égal au vrai paramètre qui nous intéresse.) Autrement dit :

\[
\mathbb{E}(\hat{\theta}) = \theta
\]

\textbf{Efficace :} Un estimateur est efficace s’il atteint la borne de Cramér-Rao. On veut l’estimateur avec la variance la plus petite possible afin d’avoir une meilleure précision. (Ceci fait référence à la variance minimale de Gauss-Markov)

Plus tard, nous définirons aussi la propriété de convergence.

\textbf{Borne de Cramér-Rao :} Pour tout estimateur régulier et sans biais, sa variance est bornée par l’inverse de la matrice d’information.

La matrice d’information est quant à elle une façon de mesurer la quantité d’information sur les paramètres dans \(\theta\) contenue dans \(X\).

Une définition équivalente serait que la variance d’un estimateur sans biais sera toujours au moins aussi grande que l’inverse de la matrice d’information :

\begin{align*}
[I(\theta)]^{-1} &= \left(-\mathbb{E} \left[ \frac{\partial^2 \ln \mathcal{L}(\theta)}{\partial \theta^2} \right] \right)^{-1} \\
&= \left(\mathbb{E} \left[ \left(\frac{\partial \ln \mathcal{L}(\theta)}{\partial \theta}\right)^2 \right] \right)^{-1}
\end{align*}

(See Appendix C of \livre{Greene} for additional information on the Cramér-Rao bound p.1059). In our case,

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta'} &= -\frac{1}{\sigma^2} (X'X) \\
\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \sigma^2} &= -\frac{1}{\sigma^4} (X'u) \\
\frac{\partial^2 \mathcal{L}}{\partial \sigma^2 \partial \beta'} &= -\frac{1}{\sigma^4} (u'X) \\
\frac{\partial^2 \mathcal{L}}{\partial (\sigma^2)^2} &= -\frac{T}{2\sigma^4} + \frac{(Y - X\beta)'(Y - X\beta)}{2\sigma^6}
\end{align*}

En prenant l'espérance mathématique de chaque dérivée, nous obtenons :

\begin{align*}
\mathbb{E} \left( -\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta'} \right) &= \frac{1}{\sigma^2} (X'X) \\
\mathbb{E} \left( -\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \sigma^2} \right) &= \frac{1}{\sigma^4} (X'\mathbb{E}(Y) - X'X\beta) = 0 \\
\mathbb{E} \left( -\frac{\partial^2 \mathcal{L}}{\partial (\sigma^2)^2} \right) &= \frac{2\sigma^4}{T}
\end{align*}

Ainsi, la matrice d’information est :

\[
I^{-1}(\beta, \sigma^2) = \begin{bmatrix}
\sigma^2 (X'X)^{-1} & 0 \\
0 & \frac{2\sigma^4}{T}
\end{bmatrix}
\]

\subsection{Calculs des éléments de la matrice d’information}

\begin{align*}
\mathbb{E} \left[ -\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta'} \right] &= \frac{1}{\sigma^2} (X'X) \\
\mathbb{E} \left[ -\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \sigma^2} \right] &= 0 \\
\mathbb{E} \left[ -\frac{\partial^2 \mathcal{L}}{\partial (\sigma^2)^2} \right] &= \frac{2\sigma^4}{T}
\end{align*}

Ainsi,

\[
I^{-1}(\beta, \sigma^2) = \begin{bmatrix}
\sigma^2 (X'X)^{-1} & 0 \\
0 & \frac{2\sigma^4}{T}
\end{bmatrix}
\]

\subsection{Propriétés de \(\hat{\beta}\)}

\begin{align*}
\hat{\beta} &= (X'X)^{-1}X'Y \\
&= (X'X)^{-1}X'(X\beta + u) \\
&= \beta + (X'X)^{-1}X'u
\end{align*}

\[
\mathbb{E}(\hat{\beta}) = \beta + (X'X)^{-1}X'\mathbb{E}(u) = \beta
\]

Donc, l’estimateur \(\hat{\beta}\) des MCO et du MLE est sans biais. Regardons maintenant si sa variance atteint la borne minimale de Cramér-Rao (s’il est efficace).

\begin{align*}
V(\hat{\beta}) = \mathbb{E}[(\hat{\beta} - \beta)(\hat{\beta} - \beta)']  & = \mathbb{E}[(X'X)^{-1}X'u u'X(X'X)^{-1}] \\
& = (X'X)^{-1}X'\mathbb{E}(u u')X(X'X)^{-1} \\
& = \sigma^2 (X'X)^{-1}
\end{align*}

Donc, \(V(\hat{\beta})\) atteint la borne de Cramér-Rao.

\subsection{Propriétés de \(\hat{\sigma}^2\)}

\begin{align*}
\mathbb{E}(\hat{\sigma}^2) &= \mathbb{E}\left( \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{T} \right) \\
&= \frac{\sigma^2 (T - K)}{T}
\end{align*}

Nous savons que :

\[
(Y - X\hat{\beta})'(Y - X\hat{\beta}) \sim \chi^2(T - K)
\]

Nous savons que les erreurs (\(u_t\)) sont normales par hypothèses, et les résidus (\(\hat{u}_t\)) aussi par hypothèse. On sait donc que cette expression (qui est la somme des carrés des résidus) suit une loi chi carré (loi normale centrée élevée au carré).

De plus, on peut montrer que :

\[
\mathbb{E}\left[ \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{\sigma^2} \right] = T - K
\]

\[
V\left( \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{\sigma^2} \right) = 2(T - K)
\]

Cette expression n’est pas exactement égale à ce que l’on cherche soit \(\hat{\sigma}^2\).

\[
\mathbb{E}(\hat{\sigma}^2) = \mathbb{E} \left( \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{T} \right) = \frac{\sigma^2 (T - K)}{T}
\]

On voit donc que cette estimation est biaisée et la borne ne s’applique pas. Toutefois, lorsque \(T\) est suffisamment grand, \(T - K \approx T\) et le biais s’annule.

Par contre, l’estimateur de la variance suivant sera sans biais :

\[
\hat{s}^2 = \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{T - K}
\]

Car :

\[
\mathbb{E}(\hat{s}^2) = \sigma^2
\]

\footnote{NB : si vous ne voulez pas faire cette hypothèse simplificatrice, vous pouvez regarder la preuve du théorème E.4 dans \livre{Wooldridge} qui utilise plutôt la matrice de projection des \(X\), \(M\).}

\subsection{Variance de \(\hat{s}^2\)}

On peut maintenant vérifier si cet estimateur de la variance atteint la borne de Cramér-Rao. (Note : \(\hat{s}^2 \neq \hat{\sigma}^2\), c’est un nouvel estimateur)

\begin{align*}
V(\hat{s}^2) = & \mathbb{E}\left[ \left( \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{T - K} \right)^2 \right] - \left( \mathbb{E}\left[ \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{T - K} \right] \right)^2 \\
&= \frac{2\sigma^4}{T - K}
\end{align*}

Ce qui n’atteint pas la borne :

\[
\frac{2\sigma^4}{T - K} > \frac{2\sigma^4}{T}
\]

Encore une fois, si \(T\) est suffisamment grand et que \(K\) reste fixe, nous atteindrons presque la borne de variance minimale.

\section{Bibliographie}
\addcontentsline{toc}{section}{Bibliographie}

\begin{thebibliography}{99}

\bibitem{GujaratiPorter}
Gujarati, D. N., \& Porter, D. C. (2009). \textit{Basic Econometrics}. McGraw-Hill.

\bibitem{Wooldridge}
Wooldridge, J. M. (2010). \textit{Econometric Analysis of Cross Section and Panel Data}. MIT Press.

\bibitem{Greene}
Greene, W. H. (2012). \textit{Econometric Analysis}. Pearson.

\end{thebibliography}

\end{document}
